{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "# Inisialisasi Sastrawi stemmer dan stopword remover\n",
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stopword_remover = stopword_factory.create_stop_word_remover()\n",
    "stopwords = stopword_factory.get_stop_words()\n",
    "\n",
    "# Fungsi untuk membaca file txt dengan deteksi encoding\n",
    "def read_txt_file(file_number):\n",
    "    filename = f'news_dataset/data{file_number}.txt'  # Sesuaikan path\n",
    "    try:\n",
    "        with open(filename, 'rb') as file:\n",
    "            raw_data = file.read()\n",
    "            encoding = chardet.detect(raw_data)['encoding']\n",
    "        \n",
    "        with open(filename, 'r', encoding=encoding) as file:\n",
    "            content = file.read()\n",
    "            if not content.strip():\n",
    "                print(f\"Warning: {filename} is empty\")\n",
    "            return content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {filename} not found\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filename}: {str(e)}\")\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                return file.read()\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "# Fungsi untuk preprocessing text (Tokenizing, Filtering, Stemming/Tagging)\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        print(f\"Warning: Input text is not string: {type(text)}\")\n",
    "        return []\n",
    "    if not text.strip():\n",
    "        print(\"Warning: Input text is empty\")\n",
    "        return []\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    if not tokens:\n",
    "        print(\"Warning: No tokens generated after preprocessing\")\n",
    "    return tokens\n",
    "\n",
    "# Fungsi untuk menghitung TF\n",
    "def calculate_tf(documents):\n",
    "    tf_scores = {}\n",
    "    for doc_id, tokens in documents.items():\n",
    "        tf_dict = {}\n",
    "        for token in tokens:\n",
    "            tf_dict[token] = tf_dict.get(token, 0) + 1\n",
    "        if tf_dict:\n",
    "            tf_scores[doc_id] = tf_dict\n",
    "    return tf_scores\n",
    "\n",
    "# Fungsi untuk memfilter TF rendah\n",
    "def filter_low_tf(tf_scores, threshold=0.5):\n",
    "    all_values = [score for doc_scores in tf_scores.values() for score in doc_scores.values()]\n",
    "    max_tf = max(all_values) if all_values else 0\n",
    "    threshold_value = max_tf * threshold\n",
    "    \n",
    "    filtered_tf = {\n",
    "        doc_id: {word: score for word, score in scores.items() if score >= threshold_value}\n",
    "        for doc_id, scores in tf_scores.items()\n",
    "    }\n",
    "    return filtered_tf\n",
    "\n",
    "# Fungsi untuk preprocessing query\n",
    "def preprocess_query(query):\n",
    "    tokens = preprocess_text(query)\n",
    "    return tokens\n",
    "\n",
    "# Fungsi untuk menghitung similarity dan ranking dokumen\n",
    "def rank_documents(query_tokens, tf_scores):\n",
    "    rankings = {}\n",
    "    for doc_id, tf_dict in tf_scores.items():\n",
    "        score = sum(tf_dict.get(token, 0) for token in query_tokens)\n",
    "        if score > 0:\n",
    "            rankings[doc_id] = score\n",
    "    \n",
    "    sorted_docs = sorted(rankings.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    return dict(sorted_docs)\n",
    "\n",
    "# Fungsi untuk menghitung recall dan precision\n",
    "def calculate_metrics(rankdocs, labels, query_category):\n",
    "    true_positives = sum(1 for doc_id in rankdocs if labels.get(f'data{doc_id}') == query_category)\n",
    "    total_relevant = sum(1 for category in labels.values() if category == query_category)\n",
    "    \n",
    "    precision = true_positives / len(rankdocs) if len(rankdocs) > 0 else 0\n",
    "    recall = true_positives / total_relevant if total_relevant > 0 else 0\n",
    "    \n",
    "    return recall, precision\n",
    "\n",
    "# Fungsi untuk plot metrics\n",
    "def plot_metrics(recall_values, precision_values):\n",
    "    ranks = range(1, len(recall_values) + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ranks, recall_values, marker='o', label='Recall')\n",
    "    plt.plot(ranks, precision_values, marker='x', label='Precision')\n",
    "    plt.xlabel('Top K Documents')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Recall and Precision at Different Top-K Levels')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "documents = {}\n",
    "for i in range(1, 51):\n",
    "    content = read_txt_file(i)\n",
    "    if content.strip():\n",
    "        tokens = preprocess_text(content)\n",
    "        if tokens:\n",
    "            documents[i] = tokens\n",
    "\n",
    "tf_scores = calculate_tf(documents)\n",
    "filtered_tf = filter_low_tf(tf_scores)\n",
    "\n",
    "# Process query\n",
    "query = \"pertumbuhan ekonomi, perkembangan pasar dan pergerakan harga saham\"\n",
    "query_tokens = preprocess_query(query)\n",
    "\n",
    "# Rank documents\n",
    "rankdocs = rank_documents(query_tokens, filtered_tf)\n",
    "\n",
    "# Load labels\n",
    "try:\n",
    "    labels_df = pd.read_csv('label.csv', header=None, names=['document_id', 'category'])\n",
    "    labels = dict(zip(labels_df['document_id'], labels_df['category']))\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: Label file not found\")\n",
    "    labels = {}\n",
    "\n",
    "# Calculate metrics at each rank\n",
    "recall_values = []\n",
    "precision_values = []\n",
    "for k in range(1, min(11, len(rankdocs) + 1)):\n",
    "    top_k_docs = dict(list(rankdocs.items())[:k])\n",
    "    recall, precision = calculate_metrics(top_k_docs, labels, \"ekonomi\")\n",
    "    recall_values.append(recall)\n",
    "    precision_values.append(precision)\n",
    "\n",
    "# Plot metrics\n",
    "if recall_values and precision_values:\n",
    "    plot_metrics(recall_values, precision_values)\n",
    "else:\n",
    "    print(\"No metrics to plot\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
